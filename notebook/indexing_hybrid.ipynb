{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f087d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics.aio import TextAnalyticsClient\n",
    "from typing import List\n",
    "from models.document import Document\n",
    "from azure.ai.inference.aio import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text import TranslatorCredential\n",
    "from azure.ai.translation.text.models import InputTextItem\n",
    "from azure.ai.translation.text.aio import TextTranslationClient\n",
    "from openai import AsyncAzureOpenAI\n",
    "from typing import Dict\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91241ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "language_endpoint=os.getenv('LANGUAGE_ENDPOINT')\n",
    "language_api_key=os.getenv('LANGUAGE_KEY')\n",
    "region=\"westus\"\n",
    "\n",
    "translation_endpoint = os.getenv('TRANSLATION_ENDPOINT')\n",
    "translation_key = os.getenv('TRANSLATION_KEY')\n",
    "translation_region = os.getenv('TRANSLATION_REGION')\n",
    "\n",
    "cohere_key = os.getenv('COHERE_KEY')\n",
    "cohere_model=os.getenv('COHERE_MODEL')\n",
    "cohere_endpoint=os.getenv('COHERE_ENDPOINT')\n",
    "\n",
    "search_endpoint = os.getenv('SEARCH_ENDPOINT')\n",
    "search_api_key = os.getenv('SEARCH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb3dd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"translated_dual\"\n",
    "\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                             index_name=index_name,\n",
    "                             credential=credential)\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(language_endpoint, AzureKeyCredential(language_api_key))\n",
    "\n",
    "credential = TranslatorCredential(translation_key, translation_region)\n",
    "text_translator = TextTranslationClient(endpoint=translation_endpoint, credential=credential)\n",
    "\n",
    "client = EmbeddingsClient(\n",
    "            endpoint=cohere_endpoint,\n",
    "            credential=AzureKeyCredential(cohere_key)\n",
    "        )\n",
    "model_name = cohere_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ef637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_language_documents(docs:list):\n",
    "    \"\"\"\n",
    "    The `get_language_documents` method is an asynchronous function that detects the language of a batch of documents using Azure Text Analytics service.\n",
    "\n",
    "    **Purpose:**\n",
    "    - Takes a list of documents and identifies the primary language for each document\n",
    "    - Returns processed results with language codes that are compatible with Cohere embeddings\n",
    "\n",
    "    **How it works:**\n",
    "\n",
    "    1. **Language Detection**: Calls Azure Text Analytics API (`text_analytics_client.detect_language()`) to analyze the documents asynchronously\n",
    "\n",
    "    2. **Result Processing**: For each document in the response:\n",
    "        - Creates a dictionary with the document's ID\n",
    "        - Handles errors: If detection failed, marks it as an error and includes error details\n",
    "        - Language code mapping: Special handling for Chinese - converts Azure's \"zh_chs\" (Chinese Simplified) to \"zh\" for Cohere compatibility\n",
    "        - For other languages, uses the ISO 639-1 language code from Azure\n",
    "\n",
    "    3. **Return Value**: Returns a list of processed documents, where each contains:\n",
    "        - `id`: Document identifier\n",
    "        - `language_code`: ISO 639-1 language code (when successful)\n",
    "        - `is_error`: Boolean flag if detection failed\n",
    "        - `error`: Error details (when applicable)\n",
    "\n",
    "    **Key Feature:**\n",
    "    The method handles the mismatch between Azure's language codes and Cohere's expected format, specifically normalizing Chinese language codes to ensure compatibility with the Cohere multilingual embedding model.\n",
    "    \"\"\"\n",
    "    documents = await text_analytics_client.detect_language(docs)    \n",
    "    processed_documents = []\n",
    "\n",
    "    # Parse all documents\n",
    "    for document in documents:\n",
    "\n",
    "        doc = {\n",
    "            \"id\": document.id            \n",
    "        }\n",
    "\n",
    "        if document.is_error:\n",
    "            doc['is_error'] = True\n",
    "            doc['error'] = document.error\n",
    "        else:\n",
    "            # Language simplified is different from our AI Services vs Cohere\n",
    "            if document.primary_language.iso6391_name == \"zh_chs\":\n",
    "                doc['language_code'] = \"zh\"\n",
    "            else:\n",
    "                doc['language_code'] = document.primary_language.iso6391_name            \n",
    "        \n",
    "        processed_documents.append(doc)\n",
    "\n",
    "    return processed_documents\n",
    "\n",
    "async def create_embeddings_cohere(documents:List[str]) -> List[float]:\n",
    "    \"\"\"Call Azure AI Inference endpoint using Github Model Cohere 3\"\"\"\n",
    "    \n",
    "    vectors:List[float] = []\n",
    "    response = await client.embed(input=documents,\n",
    "                                  model=cohere_model)\n",
    "    \n",
    "    for data in response.data:\n",
    "        vectors.append(data['embedding'])\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def csv_to_json_array(csv_file:str, output_file:str):\n",
    "    \"\"\"Convert CSV or Excel file to array of JSON objects with snake_case field names\"\"\"\n",
    "    \n",
    "    # Check file extension and read accordingly\n",
    "    if csv_file.endswith('.xlsx') or csv_file.endswith('.xls'):\n",
    "        # Read Excel file into DataFrame\n",
    "        df = pd.read_excel(csv_file)\n",
    "    else:\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Replace NaN values with empty strings\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Convert column names from \"Title Case\" to \"snake_case\"\n",
    "    def to_snake_case(name):\n",
    "        # Replace spaces with underscores and convert to lowercase\n",
    "        return name.replace(' ', '_').lower()\n",
    "    \n",
    "    # Rename all columns to snake_case\n",
    "    df.columns = [to_snake_case(col) for col in df.columns]\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries (JSON objects)\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Converted {len(data)} records from {csv_file} to JSON array\")\n",
    "    print(f\"Converted column names: {list(df.columns)}\")\n",
    "    print(\"\\nFirst record example:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "        \n",
    "    # Save JSON array to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nJSON array saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_json_array(csv_file=\"car_problems_multilingual.xlsx\",output_file=\"car_problems_multilingual.json\")\n",
    "\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb07b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "processed_documents = []\n",
    "\n",
    "for document in documents:\n",
    "\n",
    "    # Detect the language of the documents, here the maximum is 1000 documents with a size of 1 MB\n",
    "    docs.append({\n",
    "        \"id\": document[\"id\"],\n",
    "        \"text\": document['fault']\n",
    "    })\n",
    "\n",
    "\n",
    "results = await get_language_documents(docs)\n",
    "# Map the language results back to the original documents\n",
    "for result in results:\n",
    "    # Find the matching document by id\n",
    "    matching_doc = next((d for d in documents if d['id'] == result['id']), None)\n",
    "    if matching_doc and not result.get('is_error'):\n",
    "        matching_doc['language_code'] = result['language_code']\n",
    "        processed_documents.append(matching_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 documents to enrich\n"
     ]
    }
   ],
   "source": [
    "enriched_documents:List[Dict] = []\n",
    "\n",
    "text_to_embed:List[str] = []\n",
    "text_to_translate:List[InputTextItem] = []\n",
    "\n",
    "for doc in processed_documents:\n",
    "   \n",
    "   doc['original_language'] = doc['language_code']\n",
    "   text_to_embed.append(doc['fault'])\n",
    "\n",
    "   if doc['language_code'] == 'en':      \n",
    "      enriched_documents.append(doc)\n",
    "      continue\n",
    "\n",
    "   text_to_translate.append(InputTextItem(text=doc['brand']))\n",
    "   text_to_translate.append(InputTextItem(text=doc['model']))\n",
    "   text_to_translate.append(InputTextItem(text=doc['fault']))\n",
    "   text_to_translate.append(InputTextItem(text=doc['fix']))\n",
    "   \n",
    "   texts = await text_translator.translate(content=text_to_translate,to=['en'])\n",
    "      \n",
    "   doc['brand_en'] = texts[0].translations[0].text\n",
    "   doc['model_en'] = texts[1].translations[0].text\n",
    "   doc['fault_en'] = texts[2].translations[0].text\n",
    "   doc['fix_en'] = texts[3].translations[0].text\n",
    "\n",
    "   enriched_documents.append(doc)\n",
    "   text_to_translate.clear()\n",
    "\n",
    "print(f\"{len(enriched_documents)} documents to enrich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(enriched_documents,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 vectorized\n"
     ]
    }
   ],
   "source": [
    "vectorized_documents:List[Dict] = []\n",
    "\n",
    "for doc in enriched_documents:\n",
    "\n",
    "    # Here only one field to vectorize\n",
    "    if doc['language_code'] == 'en':\n",
    "      vector = await create_embeddings_cohere([doc['fault']])\n",
    "      doc['vector'] = vector[0]\n",
    "      vectorized_documents.append(doc)\n",
    "      # Add a small delay to avoid rate limiting\n",
    "      await asyncio.sleep(5)      \n",
    "      continue\n",
    "\n",
    "    vectors = await create_embeddings_cohere([doc['fault'],doc['fault_en']])\n",
    "    doc['vector'] = vectors[0]\n",
    "    doc['vector_en'] = vectors[1]\n",
    "\n",
    "    vectorized_documents.append(doc)\n",
    "\n",
    "    # Add a small delay to avoid rate limiting\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "print(f\"{len(vectorized_documents)} vectorized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80326c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove none needed columns\n",
    "for doc in vectorized_documents:\n",
    "    doc.pop('language_code',None)\n",
    "\n",
    "with open(\"documents_hybrid.json\", 'w', encoding='utf-8') as f:    \n",
    "    json.dump(vectorized_documents, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = await search_client.upload_documents(vectorized_documents)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
