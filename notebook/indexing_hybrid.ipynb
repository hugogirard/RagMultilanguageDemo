{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6671793e",
   "metadata": {},
   "source": [
    "# Multilingual Document Indexing with Hybrid Search\n",
    "\n",
    "This notebook demonstrates how to create a multilingual hybrid search index using:\n",
    "- **Azure AI Language** for language detection\n",
    "- **Azure AI Translator** for translation to English\n",
    "- **Cohere multilingual embeddings** for vector search\n",
    "- **Azure AI Search** for hybrid search (keyword + vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics.aio import TextAnalyticsClient\n",
    "from typing import List\n",
    "from models.document import Document\n",
    "from azure.ai.inference.aio import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text import TranslatorCredential\n",
    "from azure.ai.translation.text.models import InputTextItem\n",
    "from azure.ai.translation.text.aio import TextTranslationClient\n",
    "from openai import AsyncAzureOpenAI\n",
    "from typing import Dict\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e70a8",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load environment variables and configure the Azure services:\n",
    "- Language service for language detection\n",
    "- Translation service for converting non-English content\n",
    "- Cohere embeddings via Azure AI Inference\n",
    "- Azure AI Search for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91241ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "language_endpoint=os.getenv('LANGUAGE_ENDPOINT')\n",
    "language_api_key=os.getenv('LANGUAGE_KEY')\n",
    "region=\"westus\"\n",
    "\n",
    "translation_endpoint = os.getenv('TRANSLATION_ENDPOINT')\n",
    "translation_key = os.getenv('TRANSLATION_KEY')\n",
    "translation_region = os.getenv('TRANSLATION_REGION')\n",
    "\n",
    "cohere_key = os.getenv('COHERE_KEY')\n",
    "cohere_model=os.getenv('COHERE_MODEL')\n",
    "cohere_endpoint=os.getenv('COHERE_ENDPOINT')\n",
    "\n",
    "search_endpoint = os.getenv('SEARCH_ENDPOINT')\n",
    "search_api_key = os.getenv('SEARCH_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c47ed",
   "metadata": {},
   "source": [
    "## Initialize Clients\n",
    "\n",
    "Create clients for all Azure services that will be used in the indexing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3dd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"translated_dual\"\n",
    "\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                             index_name=index_name,\n",
    "                             credential=credential)\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(language_endpoint, AzureKeyCredential(language_api_key))\n",
    "\n",
    "credential = TranslatorCredential(translation_key, translation_region)\n",
    "text_translator = TextTranslationClient(endpoint=translation_endpoint, credential=credential)\n",
    "\n",
    "client = EmbeddingsClient(\n",
    "            endpoint=cohere_endpoint,\n",
    "            credential=AzureKeyCredential(cohere_key)\n",
    "        )\n",
    "model_name = cohere_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b708a",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Language Detection\n",
    "The `get_language_documents` function detects the language of each document using Azure Text Analytics and normalizes language codes for Cohere compatibility.\n",
    "\n",
    "### Embeddings Generation\n",
    "The `create_embeddings_cohere` function generates multilingual embeddings using Cohere's model via Azure AI Inference.\n",
    "\n",
    "### Data Conversion\n",
    "The `csv_to_json_array` function converts CSV/Excel files to JSON format with snake_case field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_language_documents(docs:list):\n",
    "    \"\"\"\n",
    "    The `get_language_documents` method is an asynchronous function that detects the language of a batch of documents using Azure Text Analytics service.\n",
    "\n",
    "    **Purpose:**\n",
    "    - Takes a list of documents and identifies the primary language for each document\n",
    "    - Returns processed results with language codes that are compatible with Cohere embeddings\n",
    "\n",
    "    **How it works:**\n",
    "\n",
    "    1. **Language Detection**: Calls Azure Text Analytics API (`text_analytics_client.detect_language()`) to analyze the documents asynchronously\n",
    "\n",
    "    2. **Result Processing**: For each document in the response:\n",
    "        - Creates a dictionary with the document's ID\n",
    "        - Handles errors: If detection failed, marks it as an error and includes error details\n",
    "        - Language code mapping: Special handling for Chinese - converts Azure's \"zh_chs\" (Chinese Simplified) to \"zh\" for Cohere compatibility\n",
    "        - For other languages, uses the ISO 639-1 language code from Azure\n",
    "\n",
    "    3. **Return Value**: Returns a list of processed documents, where each contains:\n",
    "        - `id`: Document identifier\n",
    "        - `language_code`: ISO 639-1 language code (when successful)\n",
    "        - `is_error`: Boolean flag if detection failed\n",
    "        - `error`: Error details (when applicable)\n",
    "\n",
    "    **Key Feature:**\n",
    "    The method handles the mismatch between Azure's language codes and Cohere's expected format, specifically normalizing Chinese language codes to ensure compatibility with the Cohere multilingual embedding model.\n",
    "    \"\"\"\n",
    "    documents = await text_analytics_client.detect_language(docs)    \n",
    "    processed_documents = []\n",
    "\n",
    "    # Parse all documents\n",
    "    for document in documents:\n",
    "\n",
    "        doc = {\n",
    "            \"id\": document.id            \n",
    "        }\n",
    "\n",
    "        if document.is_error:\n",
    "            doc['is_error'] = True\n",
    "            doc['error'] = document.error\n",
    "        else:\n",
    "            # Language simplified is different from our AI Services vs Cohere\n",
    "            if document.primary_language.iso6391_name == \"zh_chs\":\n",
    "                doc['language_code'] = \"zh\"\n",
    "            else:\n",
    "                doc['language_code'] = document.primary_language.iso6391_name            \n",
    "        \n",
    "        processed_documents.append(doc)\n",
    "\n",
    "    return processed_documents\n",
    "\n",
    "async def create_embeddings_cohere(documents:List[str]) -> List[float]:\n",
    "    \"\"\"Call Azure AI Inference endpoint using Github Model Cohere 3\"\"\"\n",
    "    \n",
    "    vectors:List[float] = []\n",
    "    response = await client.embed(input=documents,\n",
    "                                  model=cohere_model)\n",
    "    \n",
    "    for data in response.data:\n",
    "        vectors.append(data['embedding'])\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def csv_to_json_array(csv_file:str, output_file:str):\n",
    "    \"\"\"Convert CSV or Excel file to array of JSON objects with snake_case field names\"\"\"\n",
    "    \n",
    "    # Check file extension and read accordingly\n",
    "    if csv_file.endswith('.xlsx') or csv_file.endswith('.xls'):\n",
    "        # Read Excel file into DataFrame\n",
    "        df = pd.read_excel(csv_file)\n",
    "    else:\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Replace NaN values with empty strings\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Convert column names from \"Title Case\" to \"snake_case\"\n",
    "    def to_snake_case(name):\n",
    "        # Replace spaces with underscores and convert to lowercase\n",
    "        return name.replace(' ', '_').lower()\n",
    "    \n",
    "    # Rename all columns to snake_case\n",
    "    df.columns = [to_snake_case(col) for col in df.columns]\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries (JSON objects)\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Converted {len(data)} records from {csv_file} to JSON array\")\n",
    "    print(f\"Converted column names: {list(df.columns)}\")\n",
    "    print(\"\\nFirst record example:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "        \n",
    "    # Save JSON array to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nJSON array saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3820445",
   "metadata": {},
   "source": [
    "## Load Source Data\n",
    "\n",
    "Convert the multilingual car problems dataset from Excel to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_json_array(csv_file=\"car_problems_multilingual.xlsx\",output_file=\"car_problems_multilingual.json\")\n",
    "\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43bb49",
   "metadata": {},
   "source": [
    "## Step 1: Language Detection\n",
    "\n",
    "Detect the language of each document's fault description using Azure AI Language service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb07b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "processed_documents = []\n",
    "\n",
    "for document in documents:\n",
    "\n",
    "    # Detect the language of the documents, here the maximum is 1000 documents with a size of 1 MB\n",
    "    docs.append({\n",
    "        \"id\": document[\"id\"],\n",
    "        \"text\": document['fault']\n",
    "    })\n",
    "\n",
    "\n",
    "results = await get_language_documents(docs)\n",
    "# Map the language results back to the original documents\n",
    "for result in results:\n",
    "    # Find the matching document by id\n",
    "    matching_doc = next((d for d in documents if d['id'] == result['id']), None)\n",
    "    if matching_doc and not result.get('is_error'):\n",
    "        matching_doc['language_code'] = result['language_code']\n",
    "        processed_documents.append(matching_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6179482",
   "metadata": {},
   "source": [
    "## Review Detected Languages\n",
    "\n",
    "Display the processed documents with their detected language codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507d8f5",
   "metadata": {},
   "source": [
    "## Step 2: Translation\n",
    "\n",
    "Translate non-English documents to English:\n",
    "- English documents are kept as-is\n",
    "- Non-English documents get additional `_en` fields with translations\n",
    "- Fields translated: brand, model, fault, fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_documents:List[Dict] = []\n",
    "\n",
    "text_to_embed:List[str] = []\n",
    "text_to_translate:List[InputTextItem] = []\n",
    "\n",
    "for doc in processed_documents:\n",
    "   \n",
    "   doc['original_language'] = doc['language_code']\n",
    "   text_to_embed.append(doc['fault'])\n",
    "\n",
    "   if doc['language_code'] == 'en':      \n",
    "      enriched_documents.append(doc)\n",
    "      continue\n",
    "\n",
    "   text_to_translate.append(InputTextItem(text=doc['brand']))\n",
    "   text_to_translate.append(InputTextItem(text=doc['model']))\n",
    "   text_to_translate.append(InputTextItem(text=doc['fault']))\n",
    "   text_to_translate.append(InputTextItem(text=doc['fix']))\n",
    "   \n",
    "   texts = await text_translator.translate(content=text_to_translate,to=['en'])\n",
    "      \n",
    "   doc['brand_en'] = texts[0].translations[0].text\n",
    "   doc['model_en'] = texts[1].translations[0].text\n",
    "   doc['fault_en'] = texts[2].translations[0].text\n",
    "   doc['fix_en'] = texts[3].translations[0].text\n",
    "\n",
    "   enriched_documents.append(doc)\n",
    "   text_to_translate.clear()\n",
    "\n",
    "print(f\"{len(enriched_documents)} documents to enrich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f864687",
   "metadata": {},
   "source": [
    "## Review Enriched Documents\n",
    "\n",
    "Display the documents after translation enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(enriched_documents,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b94a9f",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "Create vector embeddings for hybrid search:\n",
    "- **English documents**: Single embedding from the fault field\n",
    "- **Non-English documents**: Two embeddings - one from original text, one from English translation\n",
    "\n",
    "This dual-vector approach enables cross-lingual similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_documents:List[Dict] = []\n",
    "\n",
    "for doc in enriched_documents:\n",
    "\n",
    "    # Here only one field to vectorize\n",
    "    if doc['language_code'] == 'en':\n",
    "      vector = await create_embeddings_cohere([doc['fault']])\n",
    "      doc['vector'] = vector[0]\n",
    "      vectorized_documents.append(doc)\n",
    "      # Add a small delay to avoid rate limiting\n",
    "      await asyncio.sleep(2)      \n",
    "      continue\n",
    "\n",
    "    vectors = await create_embeddings_cohere([doc['fault'],doc['fault_en']])\n",
    "    doc['vector'] = vectors[0]\n",
    "    doc['vector_en'] = vectors[1]\n",
    "\n",
    "    vectorized_documents.append(doc)\n",
    "\n",
    "    # Add a small delay to avoid rate limiting\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "print(f\"{len(vectorized_documents)} vectorized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2c2cb",
   "metadata": {},
   "source": [
    "## Save Processed Documents\n",
    "\n",
    "Save the fully processed documents with embeddings to a JSON file for review or backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80326c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove none needed columns\n",
    "for doc in vectorized_documents:\n",
    "    doc.pop('language_code',None)\n",
    "\n",
    "with open(\"documents_hybrid.json\", 'w', encoding='utf-8') as f:    \n",
    "    json.dump(vectorized_documents, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b951fc0",
   "metadata": {},
   "source": [
    "## Step 4: Upload to Azure AI Search\n",
    "\n",
    "Upload all vectorized documents to the Azure AI Search index for hybrid search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = await search_client.upload_documents(vectorized_documents)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
