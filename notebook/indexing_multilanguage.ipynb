{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390405fe",
   "metadata": {},
   "source": [
    "# Document Indexing Workflow\n",
    "\n",
    "## Important Note\n",
    "The document indexing process should be implemented using the **indexer feature** and executed in **multiple steps** for optimal performance and maintainability. \n",
    "\n",
    "This notebook demonstrates the step-by-step approach to document indexing, breaking down the process into manageable stages that can be:\n",
    "- Monitored individually\n",
    "- Debugged more easily\n",
    "- Rerun selectively if needed\n",
    "- Scaled appropriately based on document volume\n",
    "\n",
    "Each step in this notebook represents a distinct phase of the indexing pipeline, ensuring a structured and systematic approach to document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e6e199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics.aio import TextAnalyticsClient\n",
    "from azure.ai.textanalytics._models import DetectLanguageInput, DocumentError\n",
    "from openai import AsyncAzureOpenAI\n",
    "from typing import List, Dict\n",
    "from services.base_embedding_service import BaseEmbeddingService\n",
    "from factory.embedding_factory import EmbeddingFactory\n",
    "from models.document import Document\n",
    "from services.base_embedding_service import BaseEmbeddingService\n",
    "from azure.ai.inference.aio import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text.aio import TextTranslationClient\n",
    "from azure.ai.translation.text import TranslatorCredential\n",
    "from azure.ai.translation.text.models import InputTextItem\n",
    "from typing import Dict, List\n",
    "from models.document import Document\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import uuid\n",
    "import cohere\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc46028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "language_endpoint=os.getenv('LANGUAGE_ENDPOINT')\n",
    "language_api_key=os.getenv('LANGUAGE_KEY')\n",
    "openai_endpoint=os.getenv('OPENAI_ENDPOINT')\n",
    "openai_key = os.getenv('OPENAI_KEY')\n",
    "openai_embedding_deployment = os.getenv('EMBEDDING_OPENAI_DEPLOYMENT')\n",
    "region=\"westus\"\n",
    "\n",
    "# Translation Service\n",
    "translation_endpoint = os.getenv('TRANSLATION_ENDPOINT')\n",
    "translation_key = os.getenv('TRANSLATION_KEY')\n",
    "translation_region = os.getenv('TRANSLATION_REGION')\n",
    "\n",
    "# See the list of models available here\n",
    "# https://docs.cohere.com/docs/cohere-embed\n",
    "cohere_key = os.getenv('COHERE_KEY')\n",
    "cohere_model=os.getenv('COHERE_MODEL')\n",
    "cohere_endpoint=os.getenv('COHERE_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc2b96",
   "metadata": {},
   "source": [
    "### Load supported languages\n",
    "\n",
    "We use [Cohere](https://docs.cohere.com/docs/cohere-embed) since it support embedding for multiple languages here.\n",
    "\n",
    "We load the JSON files that support all the languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b72f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the official supported languages in Cohere\n",
    "path = os.path.join(\"cohere\",\"supported_languages.json\")\n",
    "\n",
    "with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "    supported_languages = json.load(f)\n",
    "\n",
    "print(supported_languages)\n",
    "\n",
    "# Create a dictionary for fast lookup: {code: description}\n",
    "language_dict = {lang[\"code\"]: lang[\"description\"] for lang in supported_languages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68d7af",
   "metadata": {},
   "source": [
    "Create two custom functions to validate if the languague is supported before\n",
    "the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ce5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a language code is supported\n",
    "def is_language_supported(language_code: str) -> bool:\n",
    "    \"\"\"Check if a language code is supported by Cohere embeddings\"\"\"\n",
    "\n",
    "    return language_code in language_dict\n",
    "\n",
    "# Function to get language description\n",
    "def get_language_description(language_code: str) -> str:\n",
    "    \"\"\"Get the description for a language code, or None if not supported\"\"\"\n",
    "    return language_dict.get(language_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad544f66",
   "metadata": {},
   "source": [
    "You want to send multiple documents to the Text Language Services to detect the language.  This service contains certains limits and this provide the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3900a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reached_size_limit(docs:list) -> bool:\n",
    "\n",
    "    number_of_documents = len(docs)\n",
    "\n",
    "    if number_of_documents >= 950 and number_of_documents < 1000:\n",
    "        return True\n",
    "\n",
    "    json_string = json.dumps(docs, ensure_ascii=False)\n",
    "    accurate_size_bytes = len(json_string.encode('utf-8'))\n",
    "\n",
    "    # Check if size exceeds 1 MB (1,048,576 bytes)\n",
    "    max_size_limit_bytes = 1 * 1024 * 1024  # 1 MB\n",
    "    # Set size limit to 700 KB (allowing room before 1 MB limit)\n",
    "    size_limit_bytes = 700 * 1024  # 700 KB\n",
    "\n",
    "    if accurate_size_bytes > size_limit_bytes and accurate_size_bytes <= max_size_limit_bytes:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadea03",
   "metadata": {},
   "source": [
    "Create Text Analytics client to extract the language code of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43aa5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analytics_client = TextAnalyticsClient(language_endpoint, AzureKeyCredential(language_api_key))\n",
    "credential = TranslatorCredential(translation_key,translation_region)\n",
    "text_translation_client = TextTranslationClient(endpoint=translation_endpoint,credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e5e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = EmbeddingsClient(\n",
    "            endpoint=cohere_endpoint,\n",
    "            credential=AzureKeyCredential(cohere_key)\n",
    "        )\n",
    "model_name = cohere_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0868b",
   "metadata": {},
   "source": [
    "#### Convert file to JSON\n",
    "\n",
    "We convert the XLSX file to JSON, by doing so we load the JSON in a dictionnary and add new columns to be able to load them in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json_array(csv_file:str, output_file:str):\n",
    "    \"\"\"Convert CSV or Excel file to array of JSON objects with snake_case field names\"\"\"\n",
    "    # Check file extension and read accordingly\n",
    "    if csv_file.endswith('.xlsx') or csv_file.endswith('.xls'):\n",
    "        # Read Excel file into DataFrame\n",
    "        df = pd.read_excel(csv_file)\n",
    "    else:\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Replace NaN values with empty strings\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Convert column names from \"Title Case\" to \"snake_case\"\n",
    "    def to_snake_case(name):\n",
    "        # Replace spaces with underscores and convert to lowercase\n",
    "        return name.replace(' ', '_').lower()\n",
    "    \n",
    "    # Rename all columns to snake_case\n",
    "    df.columns = [to_snake_case(col) for col in df.columns]\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries (JSON objects)\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Converted {len(data)} records from {csv_file} to JSON array\")\n",
    "    print(f\"Converted column names: {list(df.columns)}\")\n",
    "    print(\"\\nFirst record example:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "    \n",
    "    # Save JSON array to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nJSON array saved to: {output_file}\")\n",
    "\n",
    "csv_to_json_array(csv_file=\"car_problems_multilingual.xlsx\",output_file=\"car_problems_multilingual.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json in a dictionnary\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"{len(documents)} loaded\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d4cb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_language_documents(docs:list):\n",
    "    documents = await text_analytics_client.detect_language(docs)    \n",
    "    processed_documents = []\n",
    "\n",
    "    # Parse all documents\n",
    "    for document in documents:\n",
    "\n",
    "        doc = {\n",
    "            \"id\": document.id            \n",
    "        }\n",
    "\n",
    "        if document.is_error:\n",
    "            doc['is_error'] = True\n",
    "            doc['error'] = document.error\n",
    "        else:\n",
    "            # Language simplified is different from our AI Services vs Cohere\n",
    "            if document.primary_language.iso6391_name == \"zh_chs\":\n",
    "                doc['language_code'] = \"zh\"\n",
    "            else:\n",
    "                doc['language_code'] = document.primary_language.iso6391_name            \n",
    "        \n",
    "        processed_documents.append(doc)\n",
    "\n",
    "    return processed_documents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9b0bf",
   "metadata": {},
   "source": [
    "Here you could use the Batch Translation service to translate the complete document, this will be more performant but in this case, we want to only translate the fix in english.  For high volume it will be better to use the batch to avoid getting throttling.  THE SDK in PYTHON IT'S BETA, FOR PRODUCTION USE THE REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e36d8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def translate_documents_english(documents:list):\n",
    "    \"\"\"\n",
    "      We only translate the fault in this case since we only do a research\n",
    "      on this field.  If you want to do research on other field like a text search\n",
    "      this mean you will need to translate those field in the index\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_documents = []\n",
    "    \n",
    "    for document in documents:\n",
    "        \n",
    "        if document['language_code'] == 'en':\n",
    "            processed_documents.append(document)\n",
    "            continue\n",
    "\n",
    "        from_language = document['language_code']\n",
    "        to_language = [\"en\"]\n",
    "\n",
    "        #inputs:List[InputTextItem] = [document['fault']]\n",
    "        inputs = [{\"text\": document['fault']}]\n",
    "        response = await text_translation_client.translate(\n",
    "            content=inputs,\n",
    "            from_parameter=from_language,\n",
    "            to=to_language\n",
    "        )\n",
    "        \n",
    "        translation = response[0]\n",
    "\n",
    "        if translation:\n",
    "            document['fault'] = translation.translations[0].text\n",
    "            processed_documents.append(document)\n",
    "\n",
    "    return processed_documents\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_documents = await translate_documents_english(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(translated_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf4f087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"car_problems_english_translation.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(translated_documents, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "processed_documents = []\n",
    "\n",
    "for document in documents:\n",
    "\n",
    "    # Detect the language of the documents, here the maximum is 1000 documents with a size of 1 MB\n",
    "    docs.append({\n",
    "        \"id\": document[\"id\"],\n",
    "        \"text\": document['fault']\n",
    "    })\n",
    "\n",
    "    if reached_size_limit(docs):\n",
    "        print(f\"Reached size limits {len(docs)}\")\n",
    "        # Add something here\n",
    "        break\n",
    "\n",
    "if len(docs) > 0:\n",
    "    results = await get_language_documents(docs)\n",
    "    # Map the language results back to the original documents\n",
    "    for result in results:\n",
    "        # Find the matching document by id\n",
    "        matching_doc = next((d for d in documents if d['id'] == result['id']), None)\n",
    "        if matching_doc and not result.get('is_error'):\n",
    "            matching_doc['language_code'] = result['language_code']\n",
    "            processed_documents.append(matching_doc)\n",
    "\n",
    "print(\"all documents processed\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(processed_documents,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0641c",
   "metadata": {},
   "source": [
    "Loop each documents and validate if the language is supported by the embedding model, if not you will need to add a translation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_not_supported = []\n",
    "documents_to_embeds = []\n",
    "\n",
    "for doc in processed_documents:\n",
    "   \n",
    "   if not is_language_supported(doc['language_code']):\n",
    "      documents_not_supported.append({\n",
    "         \"id\": doc['id'],\n",
    "         \"language_code\": doc['language_code'],\n",
    "         \"language_description\": get_language_description(doc['language_code'])\n",
    "      })\n",
    "      continue\n",
    "\n",
    "   # Save supported document so the embedding can be a batch job\n",
    "   # for performance reason, for indexing the first batch this is the better \n",
    "   # options\n",
    "   documents_to_embeds.append({\n",
    "         \"id\": doc['id'],\n",
    "         \"language_code\": doc['language_code'],     \n",
    "         \"text\": doc['fault']  # Important this is the text that we want to create the embedding\n",
    "   })\n",
    "\n",
    "print(f\"Not supported documents {len(documents_not_supported)}\")\n",
    "print(f\"Documents supported {len(documents_to_embeds)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(documents_not_supported,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb0caf",
   "metadata": {},
   "source": [
    "Save the file to process for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "967457b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents_to_embed.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in documents_to_embeds:\n",
    "      f.write(json.dumps(doc,ensure_ascii=False) + '\\n')\n",
    "    #json.dump(documents_to_embeds, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff60830",
   "metadata": {},
   "source": [
    "Load the generated documents into a pydantic class, easier to manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f61102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_jsonl(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from a JSONL file into a list of Document objects.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data = json.loads(line)\n",
    "                documents.append(Document(**data))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_jsonl(\"documents_to_embed.jsonl\")\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c33e43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_embeddings(documents:List[str]) -> List[float]:\n",
    "    \"\"\"Call Azure AI Inference endpoint using Github Model Cohere 3\"\"\"\n",
    "    \n",
    "    vectors:List[float] = []\n",
    "    response = await client.embed(input=documents,\n",
    "                                  model=cohere_model)\n",
    "    \n",
    "    for data in response.data:\n",
    "        vectors.append(data['embedding'])\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0       \n",
    "number_of_documents = len(documents) - 1\n",
    "documents_to_embed:List[str] = []\n",
    "\n",
    "doc_test = [ documents[0],documents[1],documents[2] ]\n",
    "\n",
    "idx_document = 0\n",
    "\n",
    "print(len(doc_test))\n",
    "\n",
    "while idx < len(documents):\n",
    "            \n",
    "    idx+=1   \n",
    "    #print(idx)                     \n",
    "    documents_to_embed.append(documents[idx-1].text)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        vectors = await create_embeddings(documents_to_embed)\n",
    "\n",
    "        for v in vectors:\n",
    "            documents[idx_document].vector = v\n",
    "            idx_document+=1            \n",
    "\n",
    "        documents_to_embed.clear()\n",
    "\n",
    "if len(documents_to_embed) > 0:\n",
    "    vectors = await create_embeddings(documents_to_embed)\n",
    "\n",
    "    for v in vectors:\n",
    "        documents[idx_document].vector = v\n",
    "        idx_document+=1    \n",
    "\n",
    "print(f\"Documents embedded : {len(documents_to_embed)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2539a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents_with_vectors.json\", 'w', encoding='utf-8') as f:\n",
    "    # Convert all Pydantic models to dictionaries\n",
    "    json_data = [doc.model_dump() for doc in documents]\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d95cd5",
   "metadata": {},
   "source": [
    "Now add the vector to the original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_fix\n",
    "with open(\"documents_with_vectors.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    doc_with_vectors = json.load(f)\n",
    "\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    cars = json.load(f)    \n",
    "\n",
    "# Create a dictionary for fast lookup: {id: vector}\n",
    "vector_dict = {doc[\"id\"]: doc[\"vector\"] for doc in doc_with_vectors}\n",
    "\n",
    "# Add vectors to cars documents\n",
    "for car in cars:\n",
    "    car_id = car[\"id\"]\n",
    "    if car_id in vector_dict:\n",
    "        car[\"vector\"] = vector_dict[car_id]\n",
    "\n",
    "print(f\"Added vectors to {sum(1 for car in cars if 'vector' in car)} out of {len(cars)} documents\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cd4346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"car_problems_multilingual_with_vectors.json\", 'w', encoding='utf-8') as f:\n",
    "    for car in cars:\n",
    "      f.write(json.dumps(car,ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda6977",
   "metadata": {},
   "source": [
    "Now upload in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9efbc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "search_endpoint = os.getenv('SEARCH_ENDPOINT')\n",
    "search_api_key = os.getenv('SEARCH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "437d95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"cartroubleshooting\"\n",
    "\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "# Initialize the search index client\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                             index_name=index_name,\n",
    "                             credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = await search_client.upload_documents(cars)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
