{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390405fe",
   "metadata": {},
   "source": [
    "# Document Indexing Workflow\n",
    "\n",
    "## Important Note\n",
    "The document indexing process should be implemented using the **indexer feature** and executed in **multiple steps** for optimal performance and maintainability. \n",
    "\n",
    "This notebook demonstrates the step-by-step approach to document indexing, breaking down the process into manageable stages that can be:\n",
    "- Monitored individually\n",
    "- Debugged more easily\n",
    "- Rerun selectively if needed\n",
    "- Scaled appropriately based on document volume\n",
    "\n",
    "Each step in this notebook represents a distinct phase of the indexing pipeline, ensuring a structured and systematic approach to document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e6e199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics.aio import TextAnalyticsClient\n",
    "from azure.ai.textanalytics._models import DetectLanguageInput, DocumentError\n",
    "from openai import AsyncAzureOpenAI\n",
    "from typing import List, Dict\n",
    "from services.base_embedding_service import BaseEmbeddingService\n",
    "from factory.embedding_factory import EmbeddingFactory\n",
    "from models.document import Document\n",
    "from services.base_embedding_service import BaseEmbeddingService\n",
    "from azure.ai.inference.aio import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text.aio import TextTranslationClient\n",
    "from azure.ai.translation.text import TranslatorCredential\n",
    "from azure.ai.translation.text.models import InputTextItem\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from typing import Dict, List\n",
    "from models.document import Document\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import uuid\n",
    "import cohere\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "language_endpoint=os.getenv('LANGUAGE_ENDPOINT')\n",
    "language_api_key=os.getenv('LANGUAGE_KEY')\n",
    "openai_endpoint=os.getenv('OPENAI_ENDPOINT')\n",
    "openai_key = os.getenv('OPENAI_KEY')\n",
    "openai_embedding_deployment = os.getenv('EMBEDDING_OPENAI_DEPLOYMENT')\n",
    "region=\"westus\"\n",
    "\n",
    "# Translation Service\n",
    "translation_endpoint = os.getenv('TRANSLATION_ENDPOINT')\n",
    "translation_key = os.getenv('TRANSLATION_KEY')\n",
    "translation_region = os.getenv('TRANSLATION_REGION')\n",
    "\n",
    "# See the list of models available here\n",
    "# https://docs.cohere.com/docs/cohere-embed\n",
    "cohere_key = os.getenv('COHERE_KEY')\n",
    "cohere_model=os.getenv('COHERE_MODEL')\n",
    "cohere_endpoint=os.getenv('COHERE_ENDPOINT')\n",
    "\n",
    "# Search\n",
    "search_endpoint = os.getenv('SEARCH_ENDPOINT')\n",
    "search_api_key = os.getenv('SEARCH_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc2b96",
   "metadata": {},
   "source": [
    "### Load supported languages\n",
    "\n",
    "We use [Cohere](https://docs.cohere.com/docs/cohere-embed) since it support embedding for multiple languages here.\n",
    "\n",
    "We load the JSON files that support all the languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b72f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the official supported languages in Cohere\n",
    "path = os.path.join(\"cohere\",\"supported_languages.json\")\n",
    "\n",
    "with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "    supported_languages = json.load(f)\n",
    "\n",
    "print(supported_languages)\n",
    "\n",
    "# Create a dictionary for fast lookup: {code: description}\n",
    "language_dict = {lang[\"code\"]: lang[\"description\"] for lang in supported_languages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44dece9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analytics_client = TextAnalyticsClient(language_endpoint, AzureKeyCredential(language_api_key))\n",
    "credential = TranslatorCredential(translation_key,translation_region)\n",
    "text_translation_client = TextTranslationClient(endpoint=translation_endpoint,credential=credential)\n",
    "\n",
    "client = EmbeddingsClient(\n",
    "            endpoint=cohere_endpoint,\n",
    "            credential=AzureKeyCredential(cohere_key)\n",
    "        )\n",
    "model_name = cohere_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68d7af",
   "metadata": {},
   "source": [
    "Create two custom functions to validate if the languague is supported before\n",
    "the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ce5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a language code is supported\n",
    "def is_language_supported(language_code: str) -> bool:\n",
    "    \"\"\"Check if a language code is supported by Cohere embeddings\"\"\"\n",
    "\n",
    "    return language_code in language_dict\n",
    "\n",
    "# Function to get language description\n",
    "def get_language_description(language_code: str) -> str:\n",
    "    \"\"\"Get the description for a language code, or None if not supported\"\"\"\n",
    "    return language_dict.get(language_code)\n",
    "\n",
    "\n",
    "def reached_size_limit(docs:list) -> bool:\n",
    "    \"\"\"    \n",
    "    Check if document batch has reached service limits for Text Analytics Language Detection.\n",
    "    \n",
    "    The Azure Text Analytics service has the following constraints:\n",
    "    - Maximum 1000 documents per request\n",
    "    - Maximum 1 MB total request size\n",
    "    \n",
    "    This function returns True when approaching these limits to ensure safe batching.\n",
    "    \"\"\"\n",
    "    number_of_documents = len(docs)\n",
    "\n",
    "    if number_of_documents >= 950 and number_of_documents < 1000:\n",
    "        return True\n",
    "\n",
    "    json_string = json.dumps(docs, ensure_ascii=False)\n",
    "    accurate_size_bytes = len(json_string.encode('utf-8'))\n",
    "\n",
    "    # Check if size exceeds 1 MB (1,048,576 bytes)\n",
    "    max_size_limit_bytes = 1 * 1024 * 1024  # 1 MB\n",
    "    # Set size limit to 700 KB (allowing room before 1 MB limit)\n",
    "    size_limit_bytes = 700 * 1024  # 700 KB\n",
    "\n",
    "    if accurate_size_bytes > size_limit_bytes and accurate_size_bytes <= max_size_limit_bytes:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def csv_to_json_array(csv_file:str, output_file:str):\n",
    "    \"\"\"Convert CSV or Excel file to array of JSON objects with snake_case field names\"\"\"\n",
    "    \n",
    "    # Check file extension and read accordingly\n",
    "    if csv_file.endswith('.xlsx') or csv_file.endswith('.xls'):\n",
    "        # Read Excel file into DataFrame\n",
    "        df = pd.read_excel(csv_file)\n",
    "    else:\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Replace NaN values with empty strings\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Convert column names from \"Title Case\" to \"snake_case\"\n",
    "    def to_snake_case(name):\n",
    "        # Replace spaces with underscores and convert to lowercase\n",
    "        return name.replace(' ', '_').lower()\n",
    "    \n",
    "    # Rename all columns to snake_case\n",
    "    df.columns = [to_snake_case(col) for col in df.columns]\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries (JSON objects)\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Converted {len(data)} records from {csv_file} to JSON array\")\n",
    "    print(f\"Converted column names: {list(df.columns)}\")\n",
    "    print(\"\\nFirst record example:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "    \n",
    "    # Save JSON array to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nJSON array saved to: {output_file}\")\n",
    "\n",
    "\n",
    "async def get_language_documents(docs:list):\n",
    "    \"\"\"\n",
    "    The `get_language_documents` method is an asynchronous function that detects the language of a batch of documents using Azure Text Analytics service.\n",
    "\n",
    "    **Purpose:**\n",
    "    - Takes a list of documents and identifies the primary language for each document\n",
    "    - Returns processed results with language codes that are compatible with Cohere embeddings\n",
    "\n",
    "    **How it works:**\n",
    "\n",
    "    1. **Language Detection**: Calls Azure Text Analytics API (`text_analytics_client.detect_language()`) to analyze the documents asynchronously\n",
    "\n",
    "    2. **Result Processing**: For each document in the response:\n",
    "        - Creates a dictionary with the document's ID\n",
    "        - Handles errors: If detection failed, marks it as an error and includes error details\n",
    "        - Language code mapping: Special handling for Chinese - converts Azure's \"zh_chs\" (Chinese Simplified) to \"zh\" for Cohere compatibility\n",
    "        - For other languages, uses the ISO 639-1 language code from Azure\n",
    "\n",
    "    3. **Return Value**: Returns a list of processed documents, where each contains:\n",
    "        - `id`: Document identifier\n",
    "        - `language_code`: ISO 639-1 language code (when successful)\n",
    "        - `is_error`: Boolean flag if detection failed\n",
    "        - `error`: Error details (when applicable)\n",
    "\n",
    "    **Key Feature:**\n",
    "    The method handles the mismatch between Azure's language codes and Cohere's expected format, specifically normalizing Chinese language codes to ensure compatibility with the Cohere multilingual embedding model.\n",
    "    \"\"\"\n",
    "    documents = await text_analytics_client.detect_language(docs)    \n",
    "    processed_documents = []\n",
    "\n",
    "    # Parse all documents\n",
    "    for document in documents:\n",
    "\n",
    "        doc = {\n",
    "            \"id\": document.id            \n",
    "        }\n",
    "\n",
    "        if document.is_error:\n",
    "            doc['is_error'] = True\n",
    "            doc['error'] = document.error\n",
    "        else:\n",
    "            # Language simplified is different from our AI Services vs Cohere\n",
    "            if document.primary_language.iso6391_name == \"zh_chs\":\n",
    "                doc['language_code'] = \"zh\"\n",
    "            else:\n",
    "                doc['language_code'] = document.primary_language.iso6391_name            \n",
    "        \n",
    "        processed_documents.append(doc)\n",
    "\n",
    "    return processed_documents    \n",
    "\n",
    "\n",
    "def load_documents_from_jsonl(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from a JSONL file into a list of Document objects.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data = json.loads(line)\n",
    "                documents.append(Document(**data))\n",
    "    return documents\n",
    "\n",
    "async def create_embeddings(documents:List[str]) -> List[float]:\n",
    "    \"\"\"Call Azure AI Inference endpoint using Github Model Cohere 3\"\"\"\n",
    "    \n",
    "    vectors:List[float] = []\n",
    "    response = await client.embed(input=documents,\n",
    "                                  model=cohere_model)\n",
    "    \n",
    "    for data in response.data:\n",
    "        vectors.append(data['embedding'])\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0868b",
   "metadata": {},
   "source": [
    "#### Convert file to JSON\n",
    "\n",
    "We convert the XLSX file to JSON, by doing so we load the JSON in a dictionnary and add new columns to be able to load them in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_json_array(csv_file=\"car_problems_multilingual.xlsx\",output_file=\"car_problems_multilingual.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json in a dictionnary\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"{len(documents)} loaded\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596d58d",
   "metadata": {},
   "source": [
    "## Step 2: Detect Languages in Documents\n",
    "\n",
    "In this step, we'll process the documents to identify their languages using Azure Text Analytics. This is crucial for:\n",
    "\n",
    "- **Language validation**: Determining if the language is supported by our Cohere multilingual embedding model\n",
    "- **Batch processing**: The Azure Text Analytics service has limits (max 1000 documents, 1MB total size), so we process documents in batches\n",
    "- **Language code mapping**: Converting Azure's language codes to match Cohere's expected format (e.g., \"zh_chs\" → \"zh\")\n",
    "\n",
    "The cell below will:\n",
    "1. Iterate through all documents and prepare them for language detection\n",
    "2. Check batch size limits to ensure we don't exceed Azure's constraints\n",
    "3. Call the language detection API to identify each document's language\n",
    "4. Map the detected languages back to the original documents for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "processed_documents = []\n",
    "\n",
    "for document in documents:\n",
    "\n",
    "    # Detect the language of the documents, here the maximum is 1000 documents with a size of 1 MB\n",
    "    docs.append({\n",
    "        \"id\": document[\"id\"],\n",
    "        \"text\": document['fault']\n",
    "    })\n",
    "\n",
    "    if reached_size_limit(docs):\n",
    "        print(f\"Reached size limits {len(docs)}\")\n",
    "        # Add something here\n",
    "        break\n",
    "\n",
    "if len(docs) > 0:\n",
    "    results = await get_language_documents(docs)\n",
    "    # Map the language results back to the original documents\n",
    "    for result in results:\n",
    "        # Find the matching document by id\n",
    "        matching_doc = next((d for d in documents if d['id'] == result['id']), None)\n",
    "        if matching_doc and not result.get('is_error'):\n",
    "            matching_doc['language_code'] = result['language_code']\n",
    "            processed_documents.append(matching_doc)\n",
    "\n",
    "print(\"all documents processed\")       \n",
    "print(json.dumps(processed_documents,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0641c",
   "metadata": {},
   "source": [
    "Loop each documents and validate if the language is supported by the embedding model, if not you will need to add a translation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_not_supported = []\n",
    "documents_to_embeds = []\n",
    "\n",
    "for doc in processed_documents:\n",
    "   \n",
    "   if not is_language_supported(doc['language_code']):\n",
    "      documents_not_supported.append({\n",
    "         \"id\": doc['id'],\n",
    "         \"language_code\": doc['language_code'],\n",
    "         \"language_description\": get_language_description(doc['language_code'])\n",
    "      })\n",
    "      continue\n",
    "\n",
    "   # Save supported document so the embedding can be a batch job\n",
    "   # for performance reason, for indexing the first batch this is the better \n",
    "   # options\n",
    "   documents_to_embeds.append({\n",
    "         \"id\": doc['id'],\n",
    "         \"language_code\": doc['language_code'],     \n",
    "         \"text\": doc['fault']  # Important this is the text that we want to create the embedding\n",
    "   })\n",
    "\n",
    "print(f\"Not supported documents {len(documents_not_supported)}\")\n",
    "print(f\"Documents supported {len(documents_to_embeds)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb0caf",
   "metadata": {},
   "source": [
    "Save the file to process for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "967457b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents_to_embed.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in documents_to_embeds:\n",
    "      f.write(json.dumps(doc,ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_jsonl(\"documents_to_embed.jsonl\")\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87faab42",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings for Documents\n",
    "\n",
    "This step creates vector embeddings for all the documents that have supported languages. The embeddings are essential for enabling semantic search capabilities in the search index.\n",
    "\n",
    "**What this process does:**\n",
    "\n",
    "1. **Batch Processing**: Documents are processed in batches of 10 for optimal performance with the Cohere multilingual embedding model. This prevents timeout issues and manages API rate limits effectively.\n",
    "\n",
    "2. **Vector Generation**: For each batch of documents:\n",
    "    - Extracts the text content (the 'fault' field describing car problems)\n",
    "    - Calls the Azure AI Inference endpoint with Cohere's multilingual model\n",
    "    - Receives back 1024-dimensional vectors that capture the semantic meaning of each text\n",
    "\n",
    "3. **Vector Assignment**: The generated vectors are then attached to their corresponding Document objects, creating a complete representation that includes both the original text and its semantic embedding.\n",
    "\n",
    "4. **Progress Tracking**: The process uses index counters (`idx` and `idx_document`) to:\n",
    "    - Track progress through the document list\n",
    "    - Ensure vectors are correctly matched to their source documents\n",
    "    - Handle any remaining documents that don't fill a complete batch\n",
    "\n",
    "This embedding step is crucial for the indexing workflow as it transforms human-readable text into numerical representations that can be used for similarity searches, allowing users to find relevant car troubleshooting information across multiple languages using semantic meaning rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Documents embedded : 0\n"
     ]
    }
   ],
   "source": [
    "idx = 0       \n",
    "number_of_documents = len(documents) - 1\n",
    "documents_to_embed:List[str] = []\n",
    "\n",
    "doc_test = [ documents[0],documents[1],documents[2] ]\n",
    "\n",
    "idx_document = 0\n",
    "\n",
    "print(len(doc_test))\n",
    "\n",
    "while idx < len(documents):\n",
    "            \n",
    "    idx+=1   \n",
    "    #print(idx)                     \n",
    "    documents_to_embed.append(documents[idx-1].text)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        vectors = await create_embeddings(documents_to_embed)\n",
    "\n",
    "        for v in vectors:\n",
    "            documents[idx_document].vector = v\n",
    "            idx_document+=1            \n",
    "\n",
    "        documents_to_embed.clear()\n",
    "\n",
    "if len(documents_to_embed) > 0:\n",
    "    vectors = await create_embeddings(documents_to_embed)\n",
    "\n",
    "    for v in vectors:\n",
    "        documents[idx_document].vector = v\n",
    "        idx_document+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2539a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents_with_vectors.json\", 'w', encoding='utf-8') as f:\n",
    "    # Convert all Pydantic models to dictionaries\n",
    "    json_data = [doc.model_dump() for doc in documents]\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d95cd5",
   "metadata": {},
   "source": [
    "Now add the vector to the original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_fix\n",
    "with open(\"documents_with_vectors.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    doc_with_vectors = json.load(f)\n",
    "\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    cars = json.load(f)    \n",
    "\n",
    "# Create a dictionary for fast lookup: {id: vector}\n",
    "vector_dict = {doc[\"id\"]: doc[\"vector\"] for doc in doc_with_vectors}\n",
    "\n",
    "# Add vectors to cars documents\n",
    "for car in cars:\n",
    "    car_id = car[\"id\"]\n",
    "    if car_id in vector_dict:\n",
    "        car[\"vector\"] = vector_dict[car_id]\n",
    "\n",
    "print(f\"Added vectors to {sum(1 for car in cars if 'vector' in car)} out of {len(cars)} documents\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cd4346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"car_problems_multilingual_with_vectors.json\", 'w', encoding='utf-8') as f:\n",
    "    for car in cars:\n",
    "      f.write(json.dumps(car,ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda6977",
   "metadata": {},
   "source": [
    "Now upload in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "437d95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"cartroubleshooting\"\n",
    "\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "# Initialize the search index client\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                             index_name=index_name,\n",
    "                             credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22d6efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() The request is invalid. Details: The property 'problemtype' does not exist on type 'search.documentFields' or is not present in the API version '2025-09-01'. Make sure to only use property names that are defined by the type.\n",
      "Code: \n",
      "Message: The request is invalid. Details: The property 'problemtype' does not exist on type 'search.documentFields' or is not present in the API version '2025-09-01'. Make sure to only use property names that are defined by the type.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = await search_client.upload_documents(cars)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
