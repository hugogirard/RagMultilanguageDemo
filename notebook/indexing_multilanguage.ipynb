{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390405fe",
   "metadata": {},
   "source": [
    "# ğŸŒ Multilingual Document Indexing Workflow\n",
    "\n",
    "## ğŸ“Œ Important Note\n",
    "The document indexing process should be implemented using the **indexer feature** and executed in **multiple steps** for optimal performance and maintainability. \n",
    "\n",
    "This notebook demonstrates the step-by-step approach to document indexing, breaking down the process into manageable stages that can be:\n",
    "- ğŸ” Monitored individually\n",
    "- ğŸ› Debugged more easily\n",
    "- ğŸ”„ Rerun selectively if needed\n",
    "- ğŸ“ˆ Scaled appropriately based on document volume\n",
    "\n",
    "Each step in this notebook represents a distinct phase of the indexing pipeline, ensuring a structured and systematic approach to document processing.\n",
    "\n",
    "## ğŸ“Š Workflow Overview\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[ğŸ“ Multilingual Excel File] --> B[ğŸ“‹ Convert to JSON]\n",
    "    B --> C[ğŸŒ Detect Languages]\n",
    "    C --> D[âœ… Validate Language Support]\n",
    "    D --> E[ğŸ§® Generate Embeddings]\n",
    "    E --> F[ğŸ”— Merge Vectors with Data]\n",
    "    F --> G[ğŸ” Index in Azure AI Search]\n",
    "    \n",
    "    style A fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff\n",
    "    style C fill:#F5A623,stroke:#D68910,stroke-width:2px,color:#fff\n",
    "    style E fill:#9013FE,stroke:#6A0DAD,stroke-width:2px,color:#fff\n",
    "    style G fill:#50E3C2,stroke:#2ECC71,stroke-width:2px,color:#000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6fcb4",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup: Import Libraries and Initialize Clients\n",
    "\n",
    "This section imports all necessary libraries and sets up the Azure service clients needed for the indexing pipeline:\n",
    "- ğŸŒ **Azure Text Analytics**: For language detection\n",
    "- ğŸ¤– **Azure AI Inference**: For generating embeddings using Cohere's multilingual model\n",
    "- ğŸ” **Azure Cognitive Search**: For uploading documents to the search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6e199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics.aio import TextAnalyticsClient\n",
    "from typing import List\n",
    "from models.document import Document\n",
    "from azure.ai.inference.aio import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b56880",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration: Load Environment Variables\n",
    "\n",
    "Load all required API keys and endpoints from the `.env` file:\n",
    "- ğŸ”‘ Language service credentials for language detection\n",
    "- ğŸ¤– OpenAI and Cohere endpoints for embeddings\n",
    "- ğŸŒ Translation service configuration\n",
    "- ğŸ” Azure Cognitive Search credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc46028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "language_endpoint=os.getenv('LANGUAGE_ENDPOINT')\n",
    "language_api_key=os.getenv('LANGUAGE_KEY')\n",
    "openai_endpoint=os.getenv('OPENAI_ENDPOINT')\n",
    "openai_key = os.getenv('OPENAI_KEY')\n",
    "openai_embedding_deployment = os.getenv('EMBEDDING_OPENAI_DEPLOYMENT')\n",
    "region=\"westus\"\n",
    "\n",
    "# Translation Service\n",
    "translation_endpoint = os.getenv('TRANSLATION_ENDPOINT')\n",
    "translation_key = os.getenv('TRANSLATION_KEY')\n",
    "translation_region = os.getenv('TRANSLATION_REGION')\n",
    "\n",
    "# See the list of models available here\n",
    "# https://docs.cohere.com/docs/cohere-embed\n",
    "cohere_key = os.getenv('COHERE_KEY')\n",
    "cohere_model=os.getenv('COHERE_MODEL')\n",
    "cohere_endpoint=os.getenv('COHERE_ENDPOINT')\n",
    "\n",
    "# Search\n",
    "search_endpoint = os.getenv('SEARCH_ENDPOINT')\n",
    "search_api_key = os.getenv('SEARCH_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc2b96",
   "metadata": {},
   "source": [
    "### ğŸŒ Load Supported Languages\n",
    "\n",
    "We use [Cohere](https://docs.cohere.com/docs/cohere-embed) since it supports embedding for multiple languages.\n",
    "\n",
    "We load the JSON file that contains all supported languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b72f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'code': 'en', 'description': 'English'}, {'code': 'zh', 'description': 'Chinese (Simplified)'}, {'code': 'zh-cn', 'description': 'Chinese (China)'}, {'code': 'zh-tw', 'description': 'Chinese (Traditional)'}, {'code': 'es', 'description': 'Spanish'}, {'code': 'fr', 'description': 'French'}, {'code': 'de', 'description': 'German'}, {'code': 'ar', 'description': 'Arabic'}, {'code': 'hi', 'description': 'Hindi'}, {'code': 'pt', 'description': 'Portuguese'}, {'code': 'pt-br', 'description': 'Portuguese (Brazil)'}, {'code': 'it', 'description': 'Italian'}, {'code': 'ja', 'description': 'Japanese'}, {'code': 'ko', 'description': 'Korean'}, {'code': 'ru', 'description': 'Russian'}, {'code': 'tr', 'description': 'Turkish'}, {'code': 'pl', 'description': 'Polish'}, {'code': 'nl', 'description': 'Dutch'}, {'code': 'sv', 'description': 'Swedish'}, {'code': 'id', 'description': 'Indonesian'}, {'code': 'vi', 'description': 'Vietnamese'}, {'code': 'th', 'description': 'Thai'}, {'code': 'cs', 'description': 'Czech'}, {'code': 'da', 'description': 'Danish'}, {'code': 'el', 'description': 'Greek'}, {'code': 'he', 'description': 'Hebrew'}, {'code': 'hu', 'description': 'Hungarian'}, {'code': 'no', 'description': 'Norwegian'}, {'code': 'uk', 'description': 'Ukrainian'}, {'code': 'fi', 'description': 'Finnish'}, {'code': 'ro', 'description': 'Romanian'}]\n"
     ]
    }
   ],
   "source": [
    "# This is the official supported languages in Cohere\n",
    "path = os.path.join(\"cohere\",\"supported_languages.json\")\n",
    "\n",
    "with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "    supported_languages = json.load(f)\n",
    "\n",
    "print(supported_languages)\n",
    "\n",
    "# Create a dictionary for fast lookup: {code: description}\n",
    "language_dict = {lang[\"code\"]: lang[\"description\"] for lang in supported_languages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcf5a9",
   "metadata": {},
   "source": [
    "## ğŸ”Œ Initialize Service Clients\n",
    "\n",
    "Create authenticated clients for:\n",
    "- ğŸŒ **Text Analytics**: Language detection service\n",
    "- ğŸ¤– **Cohere Embeddings**: Multilingual embedding generation\n",
    "- ğŸ” **Azure Cognitive Search**: Document indexing and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44dece9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analytics_client = TextAnalyticsClient(language_endpoint, AzureKeyCredential(language_api_key))\n",
    "\n",
    "client = EmbeddingsClient(\n",
    "            endpoint=cohere_endpoint,\n",
    "            credential=AzureKeyCredential(cohere_key)\n",
    "        )\n",
    "model_name = cohere_model\n",
    "\n",
    "index_name = \"multilanguage\"\n",
    "\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "# Initialize the search index client\n",
    "search_client = SearchClient(endpoint=search_endpoint,\n",
    "                             index_name=index_name,\n",
    "                             credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68d7af",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Helper Functions\n",
    "\n",
    "Create utility functions to validate if the language is supported before embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfdcc2",
   "metadata": {},
   "source": [
    "### ğŸ“š Define Helper Functions\n",
    "\n",
    "This section defines utility functions used throughout the indexing pipeline:\n",
    "\n",
    "- **`is_language_supported()`**: âœ… Validates if a language code is supported by Cohere embeddings\n",
    "- **`get_language_description()`**: ğŸ“ Returns human-readable language names\n",
    "- **`reached_size_limit()`**: âš ï¸ Checks Azure Text Analytics batch limits (1000 docs or 1MB)\n",
    "- **`csv_to_json_array()`**: ğŸ“‹ Converts CSV/Excel files to JSON format\n",
    "- **`get_language_documents()`**: ğŸŒ Detects document languages using Azure Text Analytics\n",
    "- **`load_documents_from_jsonl()`**: ğŸ“‚ Loads documents from JSONL files\n",
    "- **`create_embeddings()`**: ğŸ§® Generates vector embeddings using Cohere model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ce5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a language code is supported\n",
    "def is_language_supported(language_code: str) -> bool:\n",
    "    \"\"\"Check if a language code is supported by Cohere embeddings\"\"\"\n",
    "\n",
    "    return language_code in language_dict\n",
    "\n",
    "# Function to get language description\n",
    "def get_language_description(language_code: str) -> str:\n",
    "    \"\"\"Get the description for a language code, or None if not supported\"\"\"\n",
    "    return language_dict.get(language_code)\n",
    "\n",
    "\n",
    "def reached_size_limit(docs:list) -> bool:\n",
    "    \"\"\"    \n",
    "    Check if document batch has reached service limits for Text Analytics Language Detection.\n",
    "    \n",
    "    The Azure Text Analytics service has the following constraints:\n",
    "    - Maximum 1000 documents per request\n",
    "    - Maximum 1 MB total request size\n",
    "    \n",
    "    This function returns True when approaching these limits to ensure safe batching.\n",
    "    \"\"\"\n",
    "    number_of_documents = len(docs)\n",
    "\n",
    "    if number_of_documents >= 950 and number_of_documents < 1000:\n",
    "        return True\n",
    "\n",
    "    json_string = json.dumps(docs, ensure_ascii=False)\n",
    "    accurate_size_bytes = len(json_string.encode('utf-8'))\n",
    "\n",
    "    # Check if size exceeds 1 MB (1,048,576 bytes)\n",
    "    max_size_limit_bytes = 1 * 1024 * 1024  # 1 MB\n",
    "    # Set size limit to 700 KB (allowing room before 1 MB limit)\n",
    "    size_limit_bytes = 700 * 1024  # 700 KB\n",
    "\n",
    "    if accurate_size_bytes > size_limit_bytes and accurate_size_bytes <= max_size_limit_bytes:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def csv_to_json_array(csv_file:str, output_file:str):\n",
    "    \"\"\"Convert CSV or Excel file to array of JSON objects with snake_case field names\"\"\"\n",
    "    \n",
    "    # Check file extension and read accordingly\n",
    "    if csv_file.endswith('.xlsx') or csv_file.endswith('.xls'):\n",
    "        # Read Excel file into DataFrame\n",
    "        df = pd.read_excel(csv_file)\n",
    "    else:\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Replace NaN values with empty strings\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Convert column names from \"Title Case\" to \"snake_case\"\n",
    "    def to_snake_case(name):\n",
    "        # Replace spaces with underscores and convert to lowercase\n",
    "        return name.replace(' ', '_').lower()\n",
    "    \n",
    "    # Rename all columns to snake_case\n",
    "    df.columns = [to_snake_case(col) for col in df.columns]\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries (JSON objects)\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Converted {len(data)} records from {csv_file} to JSON array\")\n",
    "    print(f\"Converted column names: {list(df.columns)}\")\n",
    "    print(\"\\nFirst record example:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "    \n",
    "    # Save JSON array to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nJSON array saved to: {output_file}\")\n",
    "\n",
    "\n",
    "async def get_language_documents(docs:list):\n",
    "    \"\"\"\n",
    "    The `get_language_documents` method is an asynchronous function that detects the language of a batch of documents using Azure Text Analytics service.\n",
    "\n",
    "    **Purpose:**\n",
    "    - Takes a list of documents and identifies the primary language for each document\n",
    "    - Returns processed results with language codes that are compatible with Cohere embeddings\n",
    "\n",
    "    **How it works:**\n",
    "\n",
    "    1. **Language Detection**: Calls Azure Text Analytics API (`text_analytics_client.detect_language()`) to analyze the documents asynchronously\n",
    "\n",
    "    2. **Result Processing**: For each document in the response:\n",
    "        - Creates a dictionary with the document's ID\n",
    "        - Handles errors: If detection failed, marks it as an error and includes error details\n",
    "        - Language code mapping: Special handling for Chinese - converts Azure's \"zh_chs\" (Chinese Simplified) to \"zh\" for Cohere compatibility\n",
    "        - For other languages, uses the ISO 639-1 language code from Azure\n",
    "\n",
    "    3. **Return Value**: Returns a list of processed documents, where each contains:\n",
    "        - `id`: Document identifier\n",
    "        - `language_code`: ISO 639-1 language code (when successful)\n",
    "        - `is_error`: Boolean flag if detection failed\n",
    "        - `error`: Error details (when applicable)\n",
    "\n",
    "    **Key Feature:**\n",
    "    The method handles the mismatch between Azure's language codes and Cohere's expected format, specifically normalizing Chinese language codes to ensure compatibility with the Cohere multilingual embedding model.\n",
    "    \"\"\"\n",
    "    documents = await text_analytics_client.detect_language(docs)    \n",
    "    processed_documents = []\n",
    "\n",
    "    # Parse all documents\n",
    "    for document in documents:\n",
    "\n",
    "        doc = {\n",
    "            \"id\": document.id            \n",
    "        }\n",
    "\n",
    "        if document.is_error:\n",
    "            doc['is_error'] = True\n",
    "            doc['error'] = document.error\n",
    "        else:\n",
    "            # Language simplified is different from our AI Services vs Cohere\n",
    "            if document.primary_language.iso6391_name == \"zh_chs\":\n",
    "                doc['language_code'] = \"zh\"\n",
    "            else:\n",
    "                doc['language_code'] = document.primary_language.iso6391_name            \n",
    "        \n",
    "        processed_documents.append(doc)\n",
    "\n",
    "    return processed_documents    \n",
    "\n",
    "\n",
    "def load_documents_from_jsonl(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from a JSONL file into a list of Document objects.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data = json.loads(line)\n",
    "                documents.append(Document(**data))\n",
    "    return documents\n",
    "\n",
    "async def create_embeddings(documents:List[str]) -> List[float]:\n",
    "    \"\"\"Call Azure AI Inference endpoint using Github Model Cohere 3\"\"\"\n",
    "    \n",
    "    vectors:List[float] = []\n",
    "    response = await client.embed(input=documents,\n",
    "                                  input_type=\"document\",\n",
    "                                  model=cohere_model)\n",
    "    \n",
    "    for data in response.data:\n",
    "        vectors.append(data['embedding'])\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0868b",
   "metadata": {},
   "source": [
    "### ğŸ“„ Convert File to JSON\n",
    "\n",
    "We convert the XLSX file to JSON. By doing so, we load the JSON into a dictionary and add new columns to be able to load them into the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf3362",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 1: Load Source Data\n",
    "\n",
    "Convert the source Excel file containing multilingual car problems into JSON format. This step:\n",
    "- ğŸ“– Reads the XLSX file with car problem descriptions in multiple languages\n",
    "- ğŸ”¤ Converts column names to snake_case for consistency\n",
    "- ğŸ’¾ Saves the data as a JSON array for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_json_array(csv_file=\"car_problems_multilingual.xlsx\",output_file=\"car_problems_multilingual.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e8ec5",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Load JSON Data into Memory\n",
    "\n",
    "Read the converted JSON file into a Python dictionary for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json in a dictionnary\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"{len(documents)} loaded\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596d58d",
   "metadata": {},
   "source": [
    "## ğŸŒ Step 2: Detect Languages in Documents\n",
    "\n",
    "In this step, we'll process the documents to identify their languages using Azure Text Analytics. This is crucial for:\n",
    "\n",
    "- âœ… **Language validation**: Determining if the language is supported by our Cohere multilingual embedding model\n",
    "- ğŸ“¦ **Batch processing**: The Azure Text Analytics service has limits (max 1000 documents, 1MB total size), so we process documents in batches\n",
    "- ğŸ”„ **Language code mapping**: Converting Azure's language codes to match Cohere's expected format (e.g., \"zh_chs\" â†’ \"zh\")\n",
    "\n",
    "The next cell will:\n",
    "1. ğŸ”„ Iterate through all documents and prepare them for language detection\n",
    "2. âš–ï¸ Check batch size limits to ensure we don't exceed Azure's constraints\n",
    "3. ğŸ“ Call the language detection API to identify each document's language\n",
    "4. ğŸ”— Map the detected languages back to the original documents for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "processed_documents = []\n",
    "\n",
    "for document in documents:\n",
    "\n",
    "    # Detect the language of the documents, here the maximum is 1000 documents with a size of 1 MB\n",
    "    docs.append({\n",
    "        \"id\": document[\"id\"],\n",
    "        \"text\": document['fault']\n",
    "    })\n",
    "\n",
    "    if reached_size_limit(docs):\n",
    "        print(f\"Reached size limits {len(docs)}\")\n",
    "        # Add something here\n",
    "        break\n",
    "\n",
    "if len(docs) > 0:\n",
    "    results = await get_language_documents(docs)\n",
    "    # Map the language results back to the original documents\n",
    "    for result in results:\n",
    "        # Find the matching document by id\n",
    "        matching_doc = next((d for d in documents if d['id'] == result['id']), None)\n",
    "        if matching_doc and not result.get('is_error'):\n",
    "            matching_doc['language_code'] = result['language_code']\n",
    "            processed_documents.append(matching_doc)\n",
    "\n",
    "print(\"all documents processed\")       \n",
    "print(json.dumps(processed_documents,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0641c",
   "metadata": {},
   "source": [
    "### âœ… Validate Language Support\n",
    "\n",
    "Loop through each document and validate if the language is supported by the embedding model. If not, you will need to add a translation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fc473",
   "metadata": {},
   "source": [
    "### ğŸ” Filter Documents by Language Support\n",
    "\n",
    "Filter documents into two categories:\n",
    "- âœ… **Supported**: Documents in languages that Cohere's multilingual model can embed\n",
    "- âŒ **Unsupported**: Documents that would require translation before embedding\n",
    "\n",
    "This step ensures we only process documents with compatible language codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_not_supported = []\n",
    "documents_to_embeds = []\n",
    "\n",
    "for doc in processed_documents:\n",
    "   \n",
    "   if not is_language_supported(doc['language_code']):\n",
    "      documents_not_supported.append({\n",
    "         \"id\": doc['id'],\n",
    "         \"language_code\": doc['language_code'],\n",
    "         \"language_description\": get_language_description(doc['language_code'])\n",
    "      })\n",
    "      continue\n",
    "\n",
    "   # Save supported document so the embedding can be a batch job\n",
    "   # for performance reason, for indexing the first batch this is the better \n",
    "   # options\n",
    "   documents_to_embeds.append({\n",
    "         \"id\": doc['id'],\n",
    "         \"language_code\": doc['language_code'],     \n",
    "         \"text\": doc['fault']  # Important this is the text that we want to create the embedding\n",
    "   })\n",
    "\n",
    "print(f\"Not supported documents {len(documents_not_supported)}\")\n",
    "print(f\"Documents supported {len(documents_to_embeds)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb0caf",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Save Supported Documents\n",
    "\n",
    "Save the validated documents to a JSONL file for batch embedding processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b04ac",
   "metadata": {},
   "source": [
    "### ğŸ“¤ Export to JSONL File\n",
    "\n",
    "Export the validated documents to a JSONL file for batch embedding processing. This intermediate file allows for:\n",
    "- ğŸ“ Checkpointing progress\n",
    "- ğŸ”„ Reprocessing if needed\n",
    "- ğŸ› Easier debugging of the embedding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "967457b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents_to_embed.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for doc in documents_to_embeds:\n",
    "      f.write(json.dumps(doc,ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b931ff6",
   "metadata": {},
   "source": [
    "### ğŸ“¥ Load Documents for Embedding\n",
    "\n",
    "Read the JSONL file back into Document objects, ready for vector generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72efda97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = load_documents_from_jsonl(\"documents_to_embed.jsonl\")\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87faab42",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 3: Generate Embeddings for Documents\n",
    "\n",
    "This step creates vector embeddings for all the documents that have supported languages. The embeddings are essential for enabling semantic search capabilities in the search index.\n",
    "\n",
    "**ğŸ¯ What this process does:**\n",
    "\n",
    "1. **ğŸ“¦ Batch Processing**: Documents are processed in batches of 10 for optimal performance with the Cohere multilingual embedding model. This prevents timeout issues and manages API rate limits effectively.\n",
    "\n",
    "2. **ğŸ”¢ Vector Generation**: For each batch of documents:\n",
    "    - ğŸ“ Extracts the text content (the 'fault' field describing car problems)\n",
    "    - ğŸ“ Calls the Azure AI Inference endpoint with Cohere's multilingual model\n",
    "    - ğŸ¯ Receives back 1024-dimensional vectors that capture the semantic meaning of each text\n",
    "\n",
    "3. **ğŸ”— Vector Assignment**: The generated vectors are then attached to their corresponding Document objects, creating a complete representation that includes both the original text and its semantic embedding.\n",
    "\n",
    "4. **ğŸ“Š Progress Tracking**: The process uses index counters (`idx` and `idx_document`) to:\n",
    "    - ğŸ“ˆ Track progress through the document list\n",
    "    - âœ… Ensure vectors are correctly matched to their source documents\n",
    "    - ğŸ”„ Handle any remaining documents that don't fill a complete batch\n",
    "\n",
    "This embedding step is crucial for the indexing workflow as it transforms human-readable text into numerical representations that can be used for similarity searches, allowing users to find relevant car troubleshooting information across multiple languages using semantic meaning rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a22cd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "idx = 0       \n",
    "number_of_documents = len(documents) - 1\n",
    "documents_to_embed:List[str] = []\n",
    "\n",
    "doc_test = [ documents[0],documents[1],documents[2] ]\n",
    "\n",
    "idx_document = 0\n",
    "\n",
    "print(len(doc_test))\n",
    "\n",
    "while idx < len(documents):\n",
    "            \n",
    "    idx+=1   \n",
    "    #print(idx)                     \n",
    "    documents_to_embed.append(documents[idx-1].text)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        vectors = await create_embeddings(documents_to_embed)\n",
    "\n",
    "        for v in vectors:\n",
    "            documents[idx_document].vector = v\n",
    "            idx_document+=1            \n",
    "\n",
    "        documents_to_embed.clear()\n",
    "\n",
    "if len(documents_to_embed) > 0:\n",
    "    vectors = await create_embeddings(documents_to_embed)\n",
    "\n",
    "    for v in vectors:\n",
    "        documents[idx_document].vector = v\n",
    "        idx_document+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388e242",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Save Documents with Vectors\n",
    "\n",
    "Export the documents with their generated embeddings to a JSON file for merging with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2539a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents_with_vectors.json\", 'w', encoding='utf-8') as f:\n",
    "    # Convert all Pydantic models to dictionaries\n",
    "    json_data = [doc.model_dump() for doc in documents]\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d95cd5",
   "metadata": {},
   "source": [
    "### ğŸ”— Merge Vectors with Original Data\n",
    "\n",
    "Now add the vectors to the original documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f659263",
   "metadata": {},
   "source": [
    "## ğŸ”— Step 4: Merge Vectors with Original Documents\n",
    "\n",
    "Combine the generated embeddings with the complete original dataset:\n",
    "1. ğŸ“‚ Load both the vectors file and the original car problems data\n",
    "2. ğŸ—‚ï¸ Create a lookup dictionary for efficient matching\n",
    "3. â• Add the vector field to each corresponding document\n",
    "4. âœ… Verify that all documents received their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82bf2d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added vectors to 60 out of 60 documents\n"
     ]
    }
   ],
   "source": [
    "# vector_fix\n",
    "with open(\"documents_with_vectors.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    doc_with_vectors = json.load(f)\n",
    "\n",
    "with open(\"car_problems_multilingual.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    cars = json.load(f)    \n",
    "\n",
    "# Create a dictionary for fast lookup: {id: vector}\n",
    "vector_dict = {doc[\"id\"]: doc[\"vector\"] for doc in doc_with_vectors}\n",
    "\n",
    "# Add vectors to cars documents\n",
    "for car in cars:\n",
    "    car_id = car[\"id\"]\n",
    "    if car_id in vector_dict:\n",
    "        car[\"vector\"] = vector_dict[car_id]\n",
    "\n",
    "print(f\"Added vectors to {sum(1 for car in cars if 'vector' in car)} out of {len(cars)} documents\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5640267",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Save Complete Dataset\n",
    "\n",
    "Export the final dataset with all original fields plus vector embeddings in JSONL format, ready for upload to Azure Cognitive Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cd4346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"car_problems_multilingual_with_vectors.json\", 'w', encoding='utf-8') as f:\n",
    "    for car in cars:\n",
    "      f.write(json.dumps(car,ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda6977",
   "metadata": {},
   "source": [
    "### ğŸš€ Upload to Search Index\n",
    "\n",
    "Now upload the enriched documents to the Azure Cognitive Search index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71967948",
   "metadata": {},
   "source": [
    "## ğŸ” Step 5: Upload to Azure Cognitive Search\n",
    "\n",
    "Upload the enriched documents (with embeddings) to the Azure Cognitive Search index. This final step:\n",
    "- ğŸ“¤ Pushes all documents to the `multilanguage` index\n",
    "- ğŸŒ Enables semantic search capabilities across multiple languages\n",
    "- âš ï¸ Provides error handling for upload failures\n",
    "\n",
    "The indexed documents can now be queried using:\n",
    "- ğŸ”¤ Traditional keyword search\n",
    "- ğŸ§® Vector similarity search\n",
    "- ğŸ”€ Hybrid search (combining both approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22d6efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload of new document succeeded: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = await search_client.upload_documents(cars)\n",
    "    print(\"Upload of new document succeeded: {}\".format(result[0].succeeded))\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5fdcd",
   "metadata": {},
   "source": [
    "## ğŸ‰ Summary\n",
    "\n",
    "This notebook successfully completes the multilingual document indexing pipeline:\n",
    "\n",
    "âœ… **Step 1**: Converted Excel data to JSON format  \n",
    "âœ… **Step 2**: Detected document languages using Azure Text Analytics  \n",
    "âœ… **Step 3**: Generated semantic embeddings using Cohere's multilingual model  \n",
    "âœ… **Step 4**: Merged vectors with original documents  \n",
    "âœ… **Step 5**: Uploaded enriched documents to Azure Cognitive Search  \n",
    "\n",
    "### ğŸŒŸ Results:\n",
    "\n",
    "The indexed documents are now searchable across multiple languages using semantic similarity, enabling powerful multilingual search capabilities for car troubleshooting information.\n",
    "\n",
    "### ğŸ”„ Comparison with Translation-First Approach:\n",
    "\n",
    "This approach differs from the translation-first indexing notebook:\n",
    "- ğŸŒ **Native multilingual**: Documents remain in their original languages\n",
    "- ğŸ¤– **Multilingual embeddings**: Uses Cohere's multilingual model\n",
    "- ğŸŒ **Multi-language queries**: Users can query in any supported language\n",
    "- âœ¨ **Preserves nuance**: Maintains cultural and linguistic specifics\n",
    "\n",
    "Choose this approach when:\n",
    "- ğŸ‘¥ Your users speak multiple languages\n",
    "- ğŸ¯ You want to preserve original language content\n",
    "- ğŸŒ Cross-lingual search is important\n",
    "- ğŸ“Š Language-specific nuances matter for your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
